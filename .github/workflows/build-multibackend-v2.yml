name: Build Multi-Backend

on:
    push:
        branches: [main, develop, master]
    pull_request:
        branches: [main]
    workflow_dispatch:

concurrency:
    group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
    cancel-in-progress: true

env:
    VULKAN_VERSION: 1.4.328.1

jobs:
    # ============================================================================
    # Linux x64 Builds
    # ============================================================================

    linux-x64-cpu:
        name: Linux x64 - CPU
        runs-on: ubuntu-22.04

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Build (Runtime Only)
              run: |
                  mkdir -p .publish/linux-x64/cpu
                  cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu -j $(nproc)
                  cp build/bin/libllama.so .publish/linux-x64/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-base.so .publish/linux-x64/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.so .publish/linux-x64/cpu/ 2>/dev/null || true
                  ls -lh .publish/linux-x64/cpu/

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-linux-x64-cpu
                  path: .publish/linux-x64/cpu/*.so
                  retention-days: 90

    linux-x64-vulkan:
        name: Linux x64 - Vulkan
        runs-on: ubuntu-22.04

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Get latest Vulkan SDK version
              run: |
                  echo "VULKAN_SDK_VERSION=$(curl -s https://vulkan.lunarg.com/sdk/latest/linux.txt)" >> $GITHUB_ENV

            - name: Use Vulkan SDK Cache
              uses: actions/cache@v4
              id: cache-sdk
              with:
                  path: ./vulkan_sdk
                  key: vulkan-sdk-${{ env.VULKAN_SDK_VERSION }}-${{ runner.os }}

            - name: Setup Vulkan SDK
              if: steps.cache-sdk.outputs.cache-hit != 'true'
              uses: ./.github/actions/linux-setup-vulkan
              with:
                  path: ./vulkan_sdk
                  version: ${{ env.VULKAN_SDK_VERSION }}

            - name: Build (Runtime Only)
              run: |
                  source ./vulkan_sdk/setup-env.sh
                  mkdir -p .publish/linux-x64/vulkan
                  cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF -DGGML_VULKAN=ON -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu ggml-vulkan -j $(nproc)
                  cp build/bin/libllama.so .publish/linux-x64/vulkan/ 2>/dev/null || true
                  cp build/bin/libggml-base.so .publish/linux-x64/vulkan/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.so .publish/linux-x64/vulkan/ 2>/dev/null || true
                  cp build/bin/libggml-vulkan.so .publish/linux-x64/vulkan/ 2>/dev/null || true
                  ls -lh .publish/linux-x64/vulkan/

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-linux-x64-vulkan
                  path: .publish/linux-x64/vulkan/*.so
                  retention-days: 90

    linux-x64-opencl:
        name: Linux x64 - OpenCL
        runs-on: ubuntu-22.04

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Install Dependencies
              run: |
                  sudo apt-get update
                  sudo apt-get install -y ocl-icd-opencl-dev opencl-headers

            - name: Build (Runtime Only)
              run: |
                  mkdir -p .publish/linux-x64/opencl
                  cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF -DGGML_OPENCL=ON -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu ggml-opencl -j $(nproc)
                  cp build/bin/libllama.so .publish/linux-x64/opencl/ 2>/dev/null || true
                  cp build/bin/libggml-base.so .publish/linux-x64/opencl/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.so .publish/linux-x64/opencl/ 2>/dev/null || true
                  cp build/bin/libggml-opencl.so .publish/linux-x64/opencl/ 2>/dev/null || true
                  ls -lh .publish/linux-x64/opencl/

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-linux-x64-opencl
                  path: .publish/linux-x64/opencl/*.so
                  retention-days: 90

    # ============================================================================
    # Linux ARM64 Build
    # ============================================================================

    linux-arm64-cpu:
        name: Linux ARM64 - CPU
        runs-on: ubuntu-22.04-arm

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Build (Runtime Only)
              run: |
                  mkdir -p .publish/linux-arm64/cpu
                  cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu -j $(nproc)
                  cp build/bin/libllama.so .publish/linux-arm64/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-base.so .publish/linux-arm64/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.so .publish/linux-arm64/cpu/ 2>/dev/null || true
                  ls -lh .publish/linux-arm64/cpu/

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-linux-arm64-cpu
                  path: .publish/linux-arm64/cpu/*.so
                  retention-days: 90

    # ============================================================================
    # Linux ARM32 Build (Embedded Devices)
    # ============================================================================

    linux-arm32-cpu:
        name: Linux ARM32 - CPU (Embedded)
        runs-on: ubuntu-22.04

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Install ARM32 Toolchain
              run: |
                  sudo apt-get update
                  sudo apt-get install -y gcc-arm-linux-gnueabihf g++-arm-linux-gnueabihf binutils-arm-linux-gnueabihf

            - name: Build (Runtime Only - Cross Compile)
              run: |
                  mkdir -p .publish/linux-arm32/cpu
                  cmake -B build \
                      -DCMAKE_SYSTEM_NAME=Linux \
                      -DCMAKE_SYSTEM_PROCESSOR=arm \
                      -DCMAKE_C_COMPILER=arm-linux-gnueabihf-gcc \
                      -DCMAKE_CXX_COMPILER=arm-linux-gnueabihf-g++ \
                      -DCMAKE_BUILD_TYPE=Release \
                      -DBUILD_SHARED_LIBS=ON \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF \
                      -DGGML_NATIVE=OFF \
                      -DGGML_CPU_ARM_ARCH=armv7-a \
                      -DLLAMA_CURL=OFF \
                      -DCMAKE_C_FLAGS="-march=armv7-a -mfpu=neon-vfpv4 -mfloat-abi=hard -Os" \
                      -DCMAKE_CXX_FLAGS="-march=armv7-a -mfpu=neon-vfpv4 -mfloat-abi=hard -Os"
                  cmake --build build --config Release --target llama ggml-base ggml-cpu -j $(nproc)
                  arm-linux-gnueabihf-strip --strip-unneeded build/bin/libllama.so 2>/dev/null || true
                  arm-linux-gnueabihf-strip --strip-unneeded build/bin/libggml-base.so 2>/dev/null || true
                  arm-linux-gnueabihf-strip --strip-unneeded build/bin/libggml-cpu.so 2>/dev/null || true
                  cp build/bin/libllama.so .publish/linux-arm32/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-base.so .publish/linux-arm32/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.so .publish/linux-arm32/cpu/ 2>/dev/null || true
                  ls -lh .publish/linux-arm32/cpu/
                  echo "Target: ARMv7-A with NEON, hard-float ABI (armhf)"

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-linux-arm32-cpu
                  path: .publish/linux-arm32/cpu/*.so
                  retention-days: 90

    # ============================================================================
    # Android ARM64 Builds
    # ============================================================================

    android-arm64-cpu:
        name: Android ARM64 - CPU
        runs-on: ubuntu-22.04

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Setup Android NDK
              uses: nttld/setup-ndk@v1
              with:
                  ndk-version: r26d
                  add-to-path: true
                  local-cache: true

            - name: Build (Runtime Only - Cross Compile)
              run: |
                  mkdir -p .publish/android-arm64/cpu
                  cmake -B build \
                      -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK_HOME/build/cmake/android.toolchain.cmake \
                      -DANDROID_ABI=arm64-v8a \
                      -DANDROID_PLATFORM=android-28 \
                      -DCMAKE_BUILD_TYPE=Release \
                      -DBUILD_SHARED_LIBS=ON \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF \
                      -DGGML_OPENMP=OFF \
                      -DGGML_LLAMAFILE=OFF \
                      -DLLAMA_CURL=OFF \
                      -DCMAKE_C_FLAGS="-march=armv8.7a" \
                      -DCMAKE_CXX_FLAGS="-march=armv8.7a"
                  cmake --build build --config Release --target llama ggml-base ggml-cpu -j $(nproc)
                  $ANDROID_NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-strip --strip-unneeded build/bin/libllama.so 2>/dev/null || true
                  $ANDROID_NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-strip --strip-unneeded build/bin/libggml-base.so 2>/dev/null || true
                  $ANDROID_NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-strip --strip-unneeded build/bin/libggml-cpu.so 2>/dev/null || true
                  cp build/bin/libllama.so .publish/android-arm64/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-base.so .publish/android-arm64/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.so .publish/android-arm64/cpu/ 2>/dev/null || true
                  ls -lh .publish/android-arm64/cpu/
                  echo "Target: Android ARM64 (arm64-v8a), API 28+"

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-android-arm64-cpu
                  path: .publish/android-arm64/cpu/*.so
                  retention-days: 90

    android-arm64-vulkan:
        name: Android ARM64 - Vulkan
        runs-on: ubuntu-22.04

        env:
            VULKAN_VERSION: 1.4.328.1

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Setup Android NDK
              uses: nttld/setup-ndk@v1
              with:
                  ndk-version: r26d
                  add-to-path: true
                  local-cache: true

            - name: Use Vulkan SDK Cache
              uses: actions/cache@v4
              id: cache-sdk
              with:
                  path: ./vulkan_sdk
                  key: vulkan-sdk-${{ env.VULKAN_VERSION }}-android-${{ runner.os }}

            - name: Setup Vulkan SDK
              if: steps.cache-sdk.outputs.cache-hit != 'true'
              uses: ./.github/actions/linux-setup-vulkan
              with:
                  path: ./vulkan_sdk
                  version: ${{ env.VULKAN_VERSION }}

            - name: Build (Runtime Only - Cross Compile)
              run: |
                  source ./vulkan_sdk/setup-env.sh
                  mkdir -p .publish/android-arm64/vulkan
                  cmake -B build \
                      -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK_HOME/build/cmake/android.toolchain.cmake \
                      -DANDROID_ABI=arm64-v8a \
                      -DANDROID_PLATFORM=android-28 \
                      -DCMAKE_BUILD_TYPE=Release \
                      -DBUILD_SHARED_LIBS=ON \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF \
                      -DGGML_VULKAN=ON \
                      -DGGML_OPENMP=OFF \
                      -DGGML_LLAMAFILE=OFF \
                      -DLLAMA_CURL=OFF \
                      -DCMAKE_C_FLAGS="-march=armv8.7a" \
                      -DCMAKE_CXX_FLAGS="-march=armv8.7a"
                  cmake --build build --config Release --target llama ggml-base ggml-cpu ggml-vulkan -j $(nproc)
                  $ANDROID_NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-strip --strip-unneeded build/bin/libllama.so 2>/dev/null || true
                  $ANDROID_NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-strip --strip-unneeded build/bin/libggml-base.so 2>/dev/null || true
                  $ANDROID_NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-strip --strip-unneeded build/bin/libggml-cpu.so 2>/dev/null || true
                  $ANDROID_NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-strip --strip-unneeded build/bin/libggml-vulkan.so 2>/dev/null || true
                  cp build/bin/libllama.so .publish/android-arm64/vulkan/ 2>/dev/null || true
                  cp build/bin/libggml-base.so .publish/android-arm64/vulkan/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.so .publish/android-arm64/vulkan/ 2>/dev/null || true
                  cp build/bin/libggml-vulkan.so .publish/android-arm64/vulkan/ 2>/dev/null || true
                  ls -lh .publish/android-arm64/vulkan/
                  echo "Target: Android ARM64 (arm64-v8a) with Vulkan, API 28+"

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-android-arm64-vulkan
                  path: .publish/android-arm64/vulkan/*.so
                  retention-days: 90

    # ============================================================================
    # iOS ARM64 Builds
    # ============================================================================

    ios-arm64-cpu:
        name: iOS ARM64 - CPU
        runs-on: macos-latest

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Build (Runtime Only)
              run: |
                  mkdir -p .publish/ios-arm64/cpu
                  cmake -B build -G Xcode \
                      -DCMAKE_BUILD_TYPE=Release \
                      -DBUILD_SHARED_LIBS=ON \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF \
                      -DLLAMA_CURL=OFF \
                      -DCMAKE_SYSTEM_NAME=iOS \
                      -DCMAKE_OSX_DEPLOYMENT_TARGET=14.0 \
                      -DCMAKE_OSX_ARCHITECTURES=arm64 \
                      -DCMAKE_XCODE_ATTRIBUTE_DEVELOPMENT_TEAM=ggml
                  cmake --build build --config Release --target llama ggml-base ggml-cpu -j $(sysctl -n hw.logicalcpu) -- CODE_SIGNING_ALLOWED=NO
                  cp build/bin/Release/libllama.dylib .publish/ios-arm64/cpu/ 2>/dev/null || true
                  cp build/bin/Release/libggml-base.dylib .publish/ios-arm64/cpu/ 2>/dev/null || true
                  cp build/bin/Release/libggml-cpu.dylib .publish/ios-arm64/cpu/ 2>/dev/null || true
                  ls -lh .publish/ios-arm64/cpu/
                  echo "Target: iOS ARM64 (arm64), iOS 14.0+"

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-ios-arm64-cpu
                  path: .publish/ios-arm64/cpu/*.dylib
                  retention-days: 90

    ios-arm64-metal:
        name: iOS ARM64 - Metal
        runs-on: macos-latest

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Build (Runtime Only)
              run: |
                  mkdir -p .publish/ios-arm64/metal
                  cmake -B build -G Xcode \
                      -DCMAKE_BUILD_TYPE=Release \
                      -DBUILD_SHARED_LIBS=ON \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF \
                      -DGGML_METAL=ON \
                      -DGGML_METAL_USE_BF16=ON \
                      -DGGML_METAL_EMBED_LIBRARY=ON \
                      -DLLAMA_CURL=OFF \
                      -DCMAKE_SYSTEM_NAME=iOS \
                      -DCMAKE_OSX_DEPLOYMENT_TARGET=14.0 \
                      -DCMAKE_OSX_ARCHITECTURES=arm64 \
                      -DCMAKE_XCODE_ATTRIBUTE_DEVELOPMENT_TEAM=ggml
                  cmake --build build --config Release --target llama ggml-base ggml-cpu ggml-metal -j $(sysctl -n hw.logicalcpu) -- CODE_SIGNING_ALLOWED=NO
                  cp build/bin/Release/libllama.dylib .publish/ios-arm64/metal/ 2>/dev/null || true
                  cp build/bin/Release/libggml-base.dylib .publish/ios-arm64/metal/ 2>/dev/null || true
                  cp build/bin/Release/libggml-cpu.dylib .publish/ios-arm64/metal/ 2>/dev/null || true
                  cp build/bin/Release/libggml-metal.dylib .publish/ios-arm64/metal/ 2>/dev/null || true
                  cp build/bin/Release/ggml-metal.metal .publish/ios-arm64/metal/ 2>/dev/null || true
                  ls -lh .publish/ios-arm64/metal/
                  echo "Target: iOS ARM64 (arm64) with Metal, iOS 14.0+"

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-ios-arm64-metal
                  path: .publish/ios-arm64/metal/*
                  retention-days: 90

    # ============================================================================
    # Windows x64 Builds
    # ============================================================================

    windows-x64-cpu:
        name: Windows x64 - CPU
        runs-on: windows-2022

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Build (Runtime Only)
              run: |
                  New-Item -ItemType Directory -Force -Path .publish/windows-x64/cpu
                  cmake -B build -G "Visual Studio 17 2022" -A x64 `
                      -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON `
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF `
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF `
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF `
                      -DGGML_BUILD_TOOLS=OFF -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu -j $env:NUMBER_OF_PROCESSORS
                  Copy-Item build/bin/Release/llama.dll .publish/windows-x64/cpu/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/Release/ggml-base.dll .publish/windows-x64/cpu/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/Release/ggml-cpu.dll .publish/windows-x64/cpu/ -ErrorAction SilentlyContinue
                  Get-ChildItem .publish/windows-x64/cpu/
              shell: powershell

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-windows-x64-cpu
                  path: .publish/windows-x64/cpu/*.dll
                  retention-days: 90

    windows-x64-vulkan:
        name: Windows x64 - Vulkan
        runs-on: windows-2022

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Install Vulkan SDK
              uses: ./.github/actions/install-exe
              with:
                  url: https://sdk.lunarg.com/sdk/download/${{ env.VULKAN_VERSION }}/windows/vulkansdk-windows-X64-${{ env.VULKAN_VERSION }}.exe
                  args: --accept-licenses --default-answer --confirm-command install

            - name: Setup Vulkan Environment
              run: |
                  Add-Content $env:GITHUB_ENV "VULKAN_SDK=C:\VulkanSDK\$env:VULKAN_VERSION"
                  Add-Content $env:GITHUB_PATH "C:\VulkanSDK\$env:VULKAN_VERSION\Bin"
              shell: powershell

            - name: Verify Vulkan SDK
              run: |
                  Write-Host "VULKAN_SDK: $env:VULKAN_SDK"
                  if (Test-Path "$env:VULKAN_SDK\Bin\glslc.exe") {
                      & "$env:VULKAN_SDK\Bin\glslc.exe" --version
                  } else {
                      Write-Error "glslc.exe not found"
                      exit 1
                  }
              shell: powershell

            - name: Build (Runtime Only)
              run: |
                  New-Item -ItemType Directory -Force -Path .publish/windows-x64/vulkan
                  cmake -B build -G "Visual Studio 17 2022" -A x64 `
                      -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON `
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF `
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF `
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF `
                      -DGGML_BUILD_TOOLS=OFF -DGGML_VULKAN=ON -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu ggml-vulkan -j $env:NUMBER_OF_PROCESSORS
                  Copy-Item build/bin/Release/llama.dll .publish/windows-x64/vulkan/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/Release/ggml-base.dll .publish/windows-x64/vulkan/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/Release/ggml-cpu.dll .publish/windows-x64/vulkan/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/Release/ggml-vulkan.dll .publish/windows-x64/vulkan/ -ErrorAction SilentlyContinue
                  Get-ChildItem .publish/windows-x64/vulkan/
              shell: powershell

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-windows-x64-vulkan
                  path: .publish/windows-x64/vulkan/*.dll
                  retention-days: 90

    windows-x64-opencl:
        name: Windows x64 - OpenCL
        runs-on: windows-2022

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Install OpenCL Headers
              run: |
                  git clone --depth 1 https://github.com/KhronosGroup/OpenCL-Headers.git
                  cd OpenCL-Headers
                  cmake -B build -DCMAKE_INSTALL_PREFIX="$env:RUNNER_TEMP/opencl"
                  cmake --build build --config Release --target install
                  cd ..
                  git clone --depth 1 https://github.com/KhronosGroup/OpenCL-ICD-Loader.git
                  cd OpenCL-ICD-Loader
                  cmake -B build -DCMAKE_PREFIX_PATH="$env:RUNNER_TEMP/opencl" -DCMAKE_INSTALL_PREFIX="$env:RUNNER_TEMP/opencl"
                  cmake --build build --config Release --target install
                  Add-Content $env:GITHUB_ENV "OpenCL_ROOT=$env:RUNNER_TEMP/opencl"
              shell: powershell

            - name: Build (Runtime Only)
              run: |
                  New-Item -ItemType Directory -Force -Path .publish/windows-x64/opencl
                  cmake -B build -G "Visual Studio 17 2022" -A x64 `
                      -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON `
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF `
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF `
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF `
                      -DGGML_BUILD_TOOLS=OFF -DGGML_OPENCL=ON -DLLAMA_CURL=OFF `
                      -DOpenCL_ROOT="$env:OpenCL_ROOT"
                  cmake --build build --config Release --target llama ggml-base ggml-cpu ggml-opencl -j $env:NUMBER_OF_PROCESSORS
                  Copy-Item build/bin/Release/llama.dll .publish/windows-x64/opencl/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/Release/ggml-base.dll .publish/windows-x64/opencl/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/Release/ggml-cpu.dll .publish/windows-x64/opencl/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/Release/ggml-opencl.dll .publish/windows-x64/opencl/ -ErrorAction SilentlyContinue
                  Get-ChildItem .publish/windows-x64/opencl/
              shell: powershell

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-windows-x64-opencl
                  path: .publish/windows-x64/opencl/*.dll
                  retention-days: 90

    windows-x64-rocm:
        name: Windows x64 - ROCm/HIP
        runs-on: windows-2022

        env:
            ROCM_VERSION: "6.4.2"
            HIPSDK_INSTALLER_VERSION: "25.Q3"

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Grab rocWMMA package
              run: |
                  curl -o rocwmma.deb "https://repo.radeon.com/rocm/apt/${{ env.ROCM_VERSION }}/pool/main/r/rocwmma-dev/rocwmma-dev_1.7.0.60402-120~24.04_amd64.deb"
                  7z x rocwmma.deb
                  7z x data.tar
              shell: powershell

            - name: Use ROCm Installation Cache
              uses: actions/cache@v4
              id: cache-rocm
              with:
                  path: C:\Program Files\AMD\ROCm
                  key: rocm-${{ env.HIPSDK_INSTALLER_VERSION }}-${{ runner.os }}

            - name: Setup ROCm
              if: steps.cache-rocm.outputs.cache-hit != 'true'
              uses: ./.github/actions/windows-setup-rocm
              with:
                  version: ${{ env.HIPSDK_INSTALLER_VERSION }}

            - name: Verify ROCm
              run: |
                  $clangPath = Get-ChildItem 'C:\Program Files\AMD\ROCm\*\bin\clang.exe' | Select-Object -First 1
                  if (-not $clangPath) {
                    Write-Error "ROCm installation not found"
                    exit 1
                  }
                  & $clangPath.FullName --version
              shell: powershell

            - name: Build (Runtime Only)
              run: |
                  New-Item -ItemType Directory -Force -Path .publish/windows-x64/rocm
                  $env:HIP_PATH=$(Resolve-Path 'C:\Program Files\AMD\ROCm\*\bin\clang.exe' | split-path | split-path)
                  $env:CMAKE_PREFIX_PATH="${env:HIP_PATH}"
                  cmake -G "Unix Makefiles" -B build -S . `
                    -DCMAKE_C_COMPILER="${env:HIP_PATH}\bin\clang.exe" `
                    -DCMAKE_CXX_COMPILER="${env:HIP_PATH}\bin\clang++.exe" `
                    -DCMAKE_CXX_FLAGS="-I$($PWD.Path.Replace('\', '/'))/opt/rocm-${{ env.ROCM_VERSION }}/include/" `
                    -DCMAKE_BUILD_TYPE=Release `
                    -DBUILD_SHARED_LIBS=ON `
                    -DROCM_DIR="${env:HIP_PATH}" `
                    -DGGML_HIP=ON `
                    -DGGML_HIP_ROCWMMA_FATTN=ON `
                    -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF `
                    -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF `
                    -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF `
                    -DGGML_BUILD_TOOLS=OFF -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu ggml-hip -j ${env:NUMBER_OF_PROCESSORS}
                  Copy-Item build/bin/libllama.so .publish/windows-x64/rocm/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/libggml-base.so .publish/windows-x64/rocm/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/libggml-cpu.so .publish/windows-x64/rocm/ -ErrorAction SilentlyContinue
                  Copy-Item build/bin/libggml-hip.so .publish/windows-x64/rocm/ -ErrorAction SilentlyContinue
                  Get-ChildItem .publish/windows-x64/rocm/
              shell: powershell

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-windows-x64-rocm
                  path: .publish/windows-x64/rocm/*.so
                  retention-days: 90

    # ============================================================================
    # Linux x64 ROCm Build
    # ============================================================================

    linux-x64-rocm:
        name: Linux x64 - ROCm/HIP
        runs-on: ubuntu-22.04
        container: rocm/dev-ubuntu-22.04:6.1.2

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Dependencies
              run: |
                  apt-get update
                  apt-get install -y build-essential git cmake rocblas-dev hipblas-dev rocwmma-dev

            - name: Build (Runtime Only)
              run: |
                  mkdir -p .publish/linux-x64/rocm
                  cmake -B build -S . \
                      -DCMAKE_HIP_COMPILER="$(hipconfig -l)/clang" \
                      -DCMAKE_BUILD_TYPE=Release \
                      -DBUILD_SHARED_LIBS=ON \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF \
                      -DGGML_HIP=ON \
                      -DGGML_HIP_ROCWMMA_FATTN=ON \
                      -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu ggml-hip -j $(nproc)
                  cp build/bin/libllama.so .publish/linux-x64/rocm/ 2>/dev/null || true
                  cp build/bin/libggml-base.so .publish/linux-x64/rocm/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.so .publish/linux-x64/rocm/ 2>/dev/null || true
                  cp build/bin/libggml-hip.so .publish/linux-x64/rocm/ 2>/dev/null || true
                  ls -lh .publish/linux-x64/rocm/

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-linux-x64-rocm
                  path: .publish/linux-x64/rocm/*.so
                  retention-days: 90

    # ============================================================================
    # macOS Builds
    # ============================================================================

    macos-arm64-cpu:
        name: macOS ARM64 - CPU
        runs-on: macos-14

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Build (Runtime Only)
              run: |
                  mkdir -p .publish/macos-arm64/cpu
                  cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON \
                      -DCMAKE_OSX_ARCHITECTURES=arm64 \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu -j $(sysctl -n hw.logicalcpu)
                  cp build/bin/libllama.dylib .publish/macos-arm64/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-base.dylib .publish/macos-arm64/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.dylib .publish/macos-arm64/cpu/ 2>/dev/null || true
                  ls -lh .publish/macos-arm64/cpu/

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-macos-arm64-cpu
                  path: .publish/macos-arm64/cpu/*.dylib
                  retention-days: 90

    macos-arm64-metal:
        name: macOS ARM64 - Metal
        runs-on: macos-14

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Build (Runtime Only)
              run: |
                  mkdir -p .publish/macos-arm64/metal
                  cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON \
                      -DCMAKE_OSX_ARCHITECTURES=arm64 \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF -DGGML_METAL=ON -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu ggml-metal -j $(sysctl -n hw.logicalcpu)
                  cp build/bin/libllama.dylib .publish/macos-arm64/metal/ 2>/dev/null || true
                  cp build/bin/libggml-base.dylib .publish/macos-arm64/metal/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.dylib .publish/macos-arm64/metal/ 2>/dev/null || true
                  cp build/bin/libggml-metal.dylib .publish/macos-arm64/metal/ 2>/dev/null || true
                  cp build/bin/ggml-metal.metal .publish/macos-arm64/metal/ 2>/dev/null || true
                  ls -lh .publish/macos-arm64/metal/

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-macos-arm64-metal
                  path: .publish/macos-arm64/metal/*
                  retention-days: 90

    macos-x64-cpu:
        name: macOS x64 - CPU
        runs-on: macos-13

        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Build (Runtime Only)
              run: |
                  mkdir -p .publish/macos-x64/cpu
                  cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON \
                      -DCMAKE_OSX_ARCHITECTURES=x86_64 \
                      -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF \
                      -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_TOOLS=OFF \
                      -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF \
                      -DGGML_BUILD_TOOLS=OFF -DLLAMA_CURL=OFF
                  cmake --build build --config Release --target llama ggml-base ggml-cpu -j $(sysctl -n hw.logicalcpu)
                  cp build/bin/libllama.dylib .publish/macos-x64/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-base.dylib .publish/macos-x64/cpu/ 2>/dev/null || true
                  cp build/bin/libggml-cpu.dylib .publish/macos-x64/cpu/ 2>/dev/null || true
                  ls -lh .publish/macos-x64/cpu/

            - name: Upload Artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: ggufx-macos-x64-cpu
                  path: .publish/macos-x64/cpu/*.dylib
                  retention-days: 90
